{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Model Training* dan Analisa Model"
      ],
      "metadata": {
        "id": "Aipej3U2Z9yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pengunduhan Data"
      ],
      "metadata": {
        "id": "Sx0uCWS6aCH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data yang akan kita gunakan berasal dari Kaggle.com dan akan kita unduh langsung ke dalam *notebook*. Pertama-tama kita akan siapkan Kaggle API untuk pengunduhan data."
      ],
      "metadata": {
        "id": "CUAO0rW4aDpp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "HvgyDOFUYRZK",
        "outputId": "b02178db-a996-478b-dead-bb9edeb0eee7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aac08cf8-7acf-44b0-8ce6-75272833207b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aac08cf8-7acf-44b0-8ce6-75272833207b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"faisalkengo\",\"key\":\"4de681ba9dea0e559d3e177f48270260\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian kita akan buat direktori baru untuk menyimpan *file* Kaggle API yang sudah diunduh."
      ],
      "metadata": {
        "id": "10t7T8xUbr_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Make directory named kaggle and copy kaggle.json file there.\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "g9xFL2psYW6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan ubah izin dari direktori yang dibuat."
      ],
      "metadata": {
        "id": "2ma7RGrFb94d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the permissions of the file.\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json "
      ],
      "metadata": {
        "id": "o238Fw0AYYSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah itu kita akan mengunduh *dataset* yang akan kita gunakan."
      ],
      "metadata": {
        "id": "ANv0wvnccck0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d humanbojack/yolo-brand-object-detection  --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlgDFCvlYZ0f",
        "outputId": "6c1cb0df-5c81-46fd-8d30-9c40a275c777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading yolo-brand-object-detection.zip to /content\n",
            " 99% 1.76G/1.78G [01:05<00:00, 43.7MB/s]\n",
            "100% 1.78G/1.78G [01:05<00:00, 29.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memuat *Library*"
      ],
      "metadata": {
        "id": "RixvzlbMch01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum kita memuat *library* yang akan digunakan, kita akan unduh model YOLOv5 yang akan kita gunakan untuk proses identifikasi gambar dan video. Selain itu kita juga akan memasang segala *library* yang digunakan di dalam model YOLOv5."
      ],
      "metadata": {
        "id": "G4PWy0omcoYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ultralytics/yolov5\n",
        "! pip install -qr /content/yolov5/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9g5SOYgYeOi",
        "outputId": "b5e49afd-03e1-4187-8c41-f1fe16293f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 12751, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 12751 (delta 62), reused 74 (delta 39), pack-reused 12633\u001b[K\n",
            "Receiving objects: 100% (12751/12751), 12.94 MiB | 5.20 MiB/s, done.\n",
            "Resolving deltas: 100% (8771/8771), done.\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 42.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian kita akan muat *library* yang akan kita gunakan."
      ],
      "metadata": {
        "id": "XJ-T-AnndFhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "O1xW3C_mYf5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum menjalankan kode di bawah, kita akan ubah nama folder 'test' menjadi 'val' di images dan labels secara manual."
      ],
      "metadata": {
        "id": "e5bTiDxMZltW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_data_path = '/content/640_data_augment_yolo_light/images'\n",
        "\n",
        "path_labels_train = '/content/640_data_augment_yolo_light/labels/train'\n",
        "path_labels_val = '/content/640_data_augment_yolo_light/labels/train'\n",
        "path_images_train = '/content/640_data_augment_yolo_light/images/train'\n",
        "path_images_val = '/content/640_data_augment_yolo_light/images/val'\n",
        "\n",
        "os.mkdir('./yolov5/data/train')\n",
        "os.mkdir('./yolov5/data/val')\n",
        "os.mkdir('./yolov5/data/test')\n",
        "os.mkdir('./yolov5/data/train/images')\n",
        "os.mkdir('./yolov5/data/train/labels')\n",
        "os.mkdir('./yolov5/data/test/images')\n",
        "os.mkdir('./yolov5/data/test/labels')\n",
        "os.mkdir('./yolov5/data/val/images')\n",
        "os.mkdir('./yolov5/data/val/labels')\n",
        "\n",
        "train_img = [*os.listdir(path_images_train)]\n",
        "val_img = [*os.listdir(path_images_val)]\n",
        "train_label = [*os.listdir(path_labels_train)]\n",
        "val_label = [*os.listdir(path_labels_val)]"
      ],
      "metadata": {
        "id": "w1k4XAOIYhaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan buat fungsi untuk membuka gambar yang akan diterapkan ke dalam model di dalam folder *train* dan *val*"
      ],
      "metadata": {
        "id": "WOQ6YI2GeYid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copyImages(imageList, imagepath,folder_Name):\n",
        "    for image in imageList:\n",
        "        img = Image.open(imagepath+'/'+image)\n",
        "        img1 = img.resize((640, 480))\n",
        "        _ = img1.save(\"./yolov5/data/\"+folder_Name+\"/images/\"+image)\n",
        "\n",
        "copyImages(train_img, path_images_train,\"train\")\n",
        "copyImages(val_img, path_images_val,\"val\")"
      ],
      "metadata": {
        "id": "0nAd7KAbYoUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan buat fungsi untuk membuka label dari gambar yang akan diterapkan ke dalam model di dalam folder *train* dan *val*"
      ],
      "metadata": {
        "id": "sido_8qqeqF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copyLabels(labelList, labelpath,folder_Name):\n",
        "    for label in labelList:\n",
        "        text = open(labelpath+'/'+label).read()\n",
        "        with open(\"./yolov5/data/\"+folder_Name+\"/labels/\"+label, \"w\") as file:\n",
        "          file.write(text) \n",
        "\n",
        "copyLabels(train_label, path_labels_train,\"train\")\n",
        "copyLabels(val_label, path_labels_val,\"val\")"
      ],
      "metadata": {
        "id": "iGa5Xk2aYt2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membangun, Melatih, dan Menganalisa Model"
      ],
      "metadata": {
        "id": "oouhxMzkiqhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk memulai pelatihan model, kita akan pindahkan direktori pekerjaan ke direktori model YOLOv5."
      ],
      "metadata": {
        "id": "D6ONhOMwe1Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgxK765BYwNW",
        "outputId": "21e10c13-3338-4beb-e8f3-d656f0738851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan persiapkan masukan dan keluaran dari model yang akan kita latih nanti."
      ],
      "metadata": {
        "id": "D0mX-cL8e8Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, Video, clear_output  # to display images\n",
        "import torch\n",
        "from yolov5 import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EahEyK4Y9F6",
        "outputId": "da75be25-8a4c-40ad-977e-0c3cdc8d4369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v6.2-164-g2787ad7 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 40.8/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian kita akan buat file `.yaml` yang berisi *brand* yang akan dideteksi."
      ],
      "metadata": {
        "id": "N_ebW4hlf-m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_text = \"\"\"train: data/train/images\n",
        "val: data/train/images\n",
        "\n",
        "nc: 38\n",
        "names: [\"Republic of Gamers\", \"Hello Fresh\", \"Displate\", \"KiwiCo\", \"World of Tanks\", \"Dollar Shave Club\", \"SkillShare\", \"Manscaped\", \"Rhinoshield\", \"Raid shadow legends\", \"Worlds of Warships\", \"Fruitz\", \"War Thunder\", \"Redbull\", \"Squarespace\", \"Brilliant.org\", \"Logitech\", \"DBrand\", \"Honey coupon\", \"Gorillas brand\", \"levlup\", \"Ridge wallet\", \"ExpressVPN\", \"State of Survival\", \"Coca Cola\", \"Crunchyroll\", \"Uber Eats\", \"Surfshark\", \"Corsair\", \"Lootcrate\", \"Amazon\", \"audible\", \"NordVPN\", \"GFuel\", \"Genshin Impact\", \"TunnelBear VPN\", \"Microsoft\", \"Winamax\"]\"\"\"\n",
        "\n",
        "with open(\"data/datacst.yaml\", 'w') as file:\n",
        "    file.write(yaml_text)\n",
        "\n",
        "%cat data/datacst.yaml"
      ],
      "metadata": {
        "id": "-nN9paaYY-er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan modifikasi iPython agar dapat menulis variabel."
      ],
      "metadata": {
        "id": "bU2yYjxngWbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ],
      "metadata": {
        "id": "1kezwkBxZC2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian kita akan mengatur arsitektur model YOLOv5 sebagai berikut."
      ],
      "metadata": {
        "id": "rt6iJKiYge1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writetemplate models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: 38  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "    - [10,13, 16,30, 33,23]  # P3/8\n",
        "    - [30,61, 62,45, 59,119]  # P4/16\n",
        "    - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "    [[-1, 1, Conv, [512, 1, 1]],\n",
        "    [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "    [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "    [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "    [-1, 1, Conv, [256, 1, 1]],\n",
        "    [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "    [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "    [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "    [-1, 1, Conv, [256, 3, 2]],\n",
        "    [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "    [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "    [-1, 1, Conv, [512, 3, 2]],\n",
        "    [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "    [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "    [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "    ]"
      ],
      "metadata": {
        "id": "8nm3EYCVZEOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita akan latih model tersebut (Perhatian: Perintah di bawah akan memakan waktu sekitar 4 jam)."
      ],
      "metadata": {
        "id": "3EWTK6JshKp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 64 --epochs 50 --data data/datacst.yaml --cfg models/custom_yolov5s.yaml --weights yolov5s.pt --name yolov5x_fold0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXr5KOqBZFky",
        "outputId": "585eff9d-ac50-4154-c9b5-ada5939b4518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=models/custom_yolov5s.yaml, data=data/datacst.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5x_fold0, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n",
            "From https://github.com/ultralytics/yolov5\n",
            "   a9f6885..2629dec  update/inference -> origin/update/inference\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.2-164-g2787ad7 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:03<00:00, 4.23MB/s]\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    115971  models.yolo.Detect                      [38, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "custom_YOLOv5s summary: 233 layers, 7354883 parameters, 7354883 gradients\n",
            "\n",
            "Transferred 223/369 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 59 weight(decay=0.0), 70 weight(decay=0.0005), 62 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/data/train/labels' images and labels...9339 found, 0 missing, 0 empty, 0 corrupt: 100% 9339/9339 [00:05<00:00, 1620.43it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/data/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/data/train/labels.cache' images and labels... 9339 found, 0 missing, 0 empty, 0 corrupt: 100% 9339/9339 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.01 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/yolov5x_fold0/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov5x_fold0\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49      14.4G    0.09264    0.02607    0.08938        165        640: 100% 146/146 [04:29<00:00,  1.85s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.25s/it]\n",
            "                   all       9339      10696    0.00162      0.339    0.00277   0.000879\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      14.3G    0.06895    0.02417    0.08408        157        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.28s/it]\n",
            "                   all       9339      10696    0.00327      0.728    0.00968    0.00322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      14.3G    0.05901    0.02113    0.07967        177        640: 100% 146/146 [04:22<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:58<00:00,  1.62s/it]\n",
            "                   all       9339      10696      0.191      0.126     0.0216    0.00813\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      14.3G     0.0538    0.01938    0.07288        158        640: 100% 146/146 [04:22<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.209      0.143     0.0878     0.0428\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      14.3G     0.0508    0.01785    0.06594        168        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.286      0.274      0.208     0.0979\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      14.3G    0.04938    0.01735    0.05865        162        640: 100% 146/146 [04:23<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.312      0.326      0.281      0.141\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      14.3G      0.048    0.01714    0.05357        186        640: 100% 146/146 [04:25<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:29<00:00,  1.23s/it]\n",
            "                   all       9339      10696      0.397      0.383      0.351      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      14.3G    0.04764    0.01648    0.04846        157        640: 100% 146/146 [04:23<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.506      0.463      0.457      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      14.3G    0.04671    0.01624    0.04285        157        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.508      0.484      0.487       0.26\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      14.3G    0.04618    0.01563    0.03905        161        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.581      0.581      0.592      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      14.3G    0.04491    0.01554     0.0356        152        640: 100% 146/146 [04:22<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.691      0.638       0.68      0.391\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      14.3G      0.044     0.0155    0.03289        168        640: 100% 146/146 [04:22<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.699       0.67      0.695      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      14.3G     0.0438    0.01518    0.02946        172        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.702      0.666      0.711       0.41\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      14.3G    0.04268    0.01479      0.028        145        640: 100% 146/146 [04:20<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.739      0.733      0.778      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      14.3G    0.04164    0.01461     0.0262        161        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.761      0.726      0.787      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      14.3G    0.04089    0.01446    0.02402        159        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.797      0.784      0.838      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      14.3G    0.04049     0.0143    0.02271        160        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.795      0.792      0.837      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      14.3G    0.03954    0.01395    0.02185        147        640: 100% 146/146 [04:20<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.837      0.828      0.879      0.577\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      14.3G    0.03899    0.01397    0.02063        164        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.854      0.841      0.892      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      14.3G    0.03861    0.01364     0.0199        161        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.23s/it]\n",
            "                   all       9339      10696      0.861      0.852        0.9      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      14.3G    0.03793    0.01369    0.01882        156        640: 100% 146/146 [04:20<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.866      0.856      0.904      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      14.3G    0.03765    0.01333    0.01813        194        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.866      0.848      0.898      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      14.3G    0.03709    0.01347    0.01699        171        640: 100% 146/146 [04:18<00:00,  1.77s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.859       0.85      0.906      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      14.3G    0.03609    0.01309    0.01647        160        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.895      0.887       0.93       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      14.3G    0.03611    0.01294    0.01557        147        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.889      0.898      0.934      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      14.3G     0.0351     0.0128    0.01551        159        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.885      0.897      0.935      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      14.3G    0.03469    0.01283    0.01501        179        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.902      0.905      0.945      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      14.3G    0.03376     0.0127    0.01404        171        640: 100% 146/146 [04:18<00:00,  1.77s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.913      0.917      0.947        0.7\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      14.3G    0.03356    0.01259    0.01399        159        640: 100% 146/146 [04:20<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.24s/it]\n",
            "                   all       9339      10696      0.912      0.918      0.953      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      14.3G    0.03279    0.01248    0.01309        163        640: 100% 146/146 [04:16<00:00,  1.76s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.913      0.925      0.952      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      14.3G    0.03177    0.01234    0.01277        155        640: 100% 146/146 [04:18<00:00,  1.77s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.913      0.931      0.957      0.724\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      14.3G    0.03148    0.01223    0.01245        155        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.28s/it]\n",
            "                   all       9339      10696      0.921      0.931       0.96      0.732\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      14.3G    0.03118    0.01197    0.01208        174        640: 100% 146/146 [04:23<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.933      0.938      0.963      0.746\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      14.3G    0.03087    0.01183      0.012        141        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.931      0.946      0.967      0.751\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      14.3G    0.03047    0.01191    0.01144        173        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.934      0.939      0.967      0.755\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      14.3G    0.02928    0.01164    0.01068        150        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.937      0.947      0.969      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      14.3G    0.02905    0.01146    0.01013        149        640: 100% 146/146 [04:18<00:00,  1.77s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.931      0.948      0.968      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      14.3G    0.02902    0.01151    0.01017        175        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.943      0.951      0.971      0.779\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      14.3G    0.02809    0.01145    0.00991        142        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.941      0.952      0.972      0.782\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      14.3G    0.02782    0.01119   0.009493        177        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.945      0.957      0.974      0.793\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      14.3G    0.02716    0.01111   0.009113        135        640: 100% 146/146 [04:23<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.949      0.956      0.975      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      14.3G    0.02671    0.01103   0.009129        153        640: 100% 146/146 [04:24<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696       0.95      0.956      0.976      0.798\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      14.3G    0.02603    0.01084   0.008908        155        640: 100% 146/146 [04:24<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.954      0.964      0.979      0.801\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      14.3G    0.02573    0.01067   0.008468        167        640: 100% 146/146 [04:23<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.28s/it]\n",
            "                   all       9339      10696      0.952      0.959      0.979      0.805\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      14.3G    0.02529    0.01078   0.008428        164        640: 100% 146/146 [04:24<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696       0.95      0.961      0.979      0.814\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      14.3G    0.02498    0.01069   0.008144        161        640: 100% 146/146 [04:23<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:33<00:00,  1.29s/it]\n",
            "                   all       9339      10696      0.951       0.96      0.979      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      14.3G    0.02419    0.01059   0.008059        158        640: 100% 146/146 [04:21<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:30<00:00,  1.25s/it]\n",
            "                   all       9339      10696      0.954      0.965       0.98      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      14.3G    0.02376    0.01034   0.007755        169        640: 100% 146/146 [04:19<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.954      0.964       0.98      0.823\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      14.3G    0.02332    0.01034   0.007426        156        640: 100% 146/146 [04:20<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:31<00:00,  1.26s/it]\n",
            "                   all       9339      10696      0.953      0.966       0.98      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      14.3G    0.02259    0.01029   0.006974        164        640: 100% 146/146 [04:24<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:32<00:00,  1.27s/it]\n",
            "                   all       9339      10696      0.953      0.965       0.98      0.828\n",
            "\n",
            "50 epochs completed in 4.931 hours.\n",
            "Optimizer stripped from runs/train/yolov5x_fold0/weights/last.pt, 15.1MB\n",
            "Optimizer stripped from runs/train/yolov5x_fold0/weights/best.pt, 15.1MB\n",
            "\n",
            "Validating runs/train/yolov5x_fold0/weights/best.pt...\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 182 layers, 7346307 parameters, 0 gradients\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 73/73 [01:35<00:00,  1.31s/it]\n",
            "                   all       9339      10696      0.952      0.965       0.98      0.828\n",
            "    Republic of Gamers       9339        285      0.947      0.947      0.973      0.809\n",
            "           Hello Fresh       9339        255      0.931      0.953      0.979      0.852\n",
            "              Displate       9339        229      0.998          1      0.995      0.917\n",
            "                KiwiCo       9339        232      0.998          1      0.995      0.934\n",
            "        World of Tanks       9339        257      0.906      0.898       0.96      0.794\n",
            "     Dollar Shave Club       9339        267      0.998          1      0.995      0.891\n",
            "            SkillShare       9339        247      0.844      0.899      0.933      0.794\n",
            "             Manscaped       9339        396      0.927      0.957      0.964       0.69\n",
            "           Rhinoshield       9339        224      0.963      0.933      0.966      0.861\n",
            "   Raid shadow legends       9339        227      0.983      0.991      0.995      0.899\n",
            "    Worlds of Warships       9339        289      0.986      0.997      0.994      0.844\n",
            "                Fruitz       9339        221      0.905      0.946      0.977      0.792\n",
            "           War Thunder       9339        256      0.985      0.994      0.994      0.865\n",
            "               Redbull       9339        360      0.916      0.983      0.985       0.84\n",
            "           Squarespace       9339        269      0.914      0.943      0.961      0.836\n",
            "         Brilliant.org       9339        252      0.904      0.885      0.926      0.718\n",
            "              Logitech       9339        283      0.983      0.995      0.995      0.863\n",
            "                DBrand       9339        263      0.924      0.962      0.988      0.847\n",
            "          Honey coupon       9339        266      0.999          1      0.995      0.905\n",
            "        Gorillas brand       9339        435      0.925      0.995      0.993      0.783\n",
            "                levlup       9339        441      0.928      0.943       0.97      0.739\n",
            "          Ridge wallet       9339        244      0.996      0.995      0.995       0.79\n",
            "            ExpressVPN       9339        258      0.957       0.96       0.98      0.812\n",
            "     State of Survival       9339        256      0.985      0.992      0.995      0.882\n",
            "             Coca Cola       9339        336      0.941       0.97      0.965       0.76\n",
            "           Crunchyroll       9339        248      0.942      0.923      0.967       0.82\n",
            "             Uber Eats       9339        315      0.974      0.984      0.991      0.829\n",
            "             Surfshark       9339        319      0.963      0.978      0.992      0.839\n",
            "               Corsair       9339        234      0.991      0.983      0.994      0.811\n",
            "             Lootcrate       9339        285      0.931      0.951      0.979      0.784\n",
            "                Amazon       9339        374      0.981      0.987      0.993      0.833\n",
            "               audible       9339        316      0.953      0.956      0.971      0.841\n",
            "               NordVPN       9339        258        0.9      0.938      0.951      0.805\n",
            "                 GFuel       9339        288      0.994       0.99      0.995        0.8\n",
            "        Genshin Impact       9339        209      0.997      0.986      0.991      0.868\n",
            "        TunnelBear VPN       9339        257      0.941      0.969      0.989      0.888\n",
            "             Microsoft       9339        271      0.964      0.977      0.988      0.812\n",
            "               Winamax       9339        274      0.914      0.929      0.966      0.821\n",
            "Results saved to \u001b[1mruns/train/yolov5x_fold0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan pelatihan model sebanyak 50 *epoch*, kita mendapati nilai presisi dan *recall* dari setiap model yang dilatih sudah berada pada rentang 90-100%. Meskipun begitu, akurasi rata-rata model untuk menentukan *brand* tertentu dengan interval yang sangat tinggi masih berada pada rentang 80%-90%. Model ini sudah merupakan model yang *good-fit* terhadap *dataset* yang kita gunakan, namun model ini masih bisa diperbaiki dengan menambah jumlah *epoch* yang akan digunakan.\n",
        "\n",
        "Kemudian kita akan simpan model terbaik yang sudah dilatih."
      ],
      "metadata": {
        "id": "Dzphn23shctW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --weights runs/train/yolov5x_fold0/weights/best.pt --include tflite --img 640"
      ],
      "metadata": {
        "id": "qJPaYXzVZI2q",
        "outputId": "de499a58-eab0-4051-e24e-8a8984c84aa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['runs/train/yolov5x_fold0/weights/best.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['tflite']\n",
            "YOLOv5 ðŸš€ v6.2-164-g2787ad7 Python-3.7.14 torch-1.12.1+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 182 layers, 7346307 parameters, 0 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train/yolov5x_fold0/weights/best.pt with output shape (1, 25200, 43) (14.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.8.2...\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "2022-09-25 19:12:22.796598: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    115971  models.yolo.Detect                      [38, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512], [640, 640]]\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(1, 640, 640, 3)]   0           []                               \n",
            "                                                                                                  \n",
            " tf_focus (TFFocus)             (1, 320, 320, 32)    3488        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf_conv_1 (TFConv)             (1, 160, 160, 64)    18496       ['tf_focus[0][0]']               \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp (TFBottlenec  (1, 160, 160, 64)   19872       ['tf_conv_1[0][0]']              \n",
            " kCSP)                                                                                            \n",
            "                                                                                                  \n",
            " tf_conv_6 (TFConv)             (1, 80, 80, 128)     73856       ['tf_bottleneck_csp[0][0]']      \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_1 (TFBottlen  (1, 80, 80, 128)    160832      ['tf_conv_6[0][0]']              \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_15 (TFConv)            (1, 40, 40, 256)     295168      ['tf_bottleneck_csp_1[0][0]']    \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_2 (TFBottlen  (1, 40, 40, 256)    641152      ['tf_conv_15[0][0]']             \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_24 (TFConv)            (1, 20, 20, 512)     1180160     ['tf_bottleneck_csp_2[0][0]']    \n",
            "                                                                                                  \n",
            " tfspp (TFSPP)                  (1, 20, 20, 512)     656128      ['tf_conv_24[0][0]']             \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_3 (TFBottlen  (1, 20, 20, 512)    1248512     ['tfspp[0][0]']                  \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_31 (TFConv)            (1, 20, 20, 256)     131328      ['tf_bottleneck_csp_3[0][0]']    \n",
            "                                                                                                  \n",
            " tf_upsample (TFUpsample)       (1, 40, 40, 256)     0           ['tf_conv_31[0][0]']             \n",
            "                                                                                                  \n",
            " tf_concat (TFConcat)           (1, 40, 40, 512)     0           ['tf_upsample[0][0]',            \n",
            "                                                                  'tf_bottleneck_csp_2[0][0]']    \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_4 (TFBottlen  (1, 40, 40, 256)    378496      ['tf_concat[0][0]']              \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_36 (TFConv)            (1, 40, 40, 128)     32896       ['tf_bottleneck_csp_4[0][0]']    \n",
            "                                                                                                  \n",
            " tf_upsample_1 (TFUpsample)     (1, 80, 80, 128)     0           ['tf_conv_36[0][0]']             \n",
            "                                                                                                  \n",
            " tf_concat_1 (TFConcat)         (1, 80, 80, 256)     0           ['tf_upsample_1[0][0]',          \n",
            "                                                                  'tf_bottleneck_csp_1[0][0]']    \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_5 (TFBottlen  (1, 80, 80, 128)    95040       ['tf_concat_1[0][0]']            \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_41 (TFConv)            (1, 40, 40, 128)     147584      ['tf_bottleneck_csp_5[0][0]']    \n",
            "                                                                                                  \n",
            " tf_concat_2 (TFConcat)         (1, 40, 40, 256)     0           ['tf_conv_41[0][0]',             \n",
            "                                                                  'tf_conv_36[0][0]']             \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_6 (TFBottlen  (1, 40, 40, 256)    312960      ['tf_concat_2[0][0]']            \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_conv_46 (TFConv)            (1, 20, 20, 256)     590080      ['tf_bottleneck_csp_6[0][0]']    \n",
            "                                                                                                  \n",
            " tf_concat_3 (TFConcat)         (1, 20, 20, 512)     0           ['tf_conv_46[0][0]',             \n",
            "                                                                  'tf_conv_31[0][0]']             \n",
            "                                                                                                  \n",
            " tf_bottleneck_csp_7 (TFBottlen  (1, 20, 20, 512)    1248512     ['tf_concat_3[0][0]']            \n",
            " eckCSP)                                                                                          \n",
            "                                                                                                  \n",
            " tf_detect (TFDetect)           ((1, 25200, 43),     115971      ['tf_bottleneck_csp_5[0][0]',    \n",
            "                                )                                 'tf_bottleneck_csp_6[0][0]',    \n",
            "                                                                  'tf_bottleneck_csp_7[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,350,531\n",
            "Trainable params: 0\n",
            "Non-trainable params: 7,350,531\n",
            "__________________________________________________________________________________________________\n",
            "2022-09-25 19:12:28.987808: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "Assets written to: runs/train/yolov5x_fold0/weights/best_saved_model/assets\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 10.2s, saved as runs/train/yolov5x_fold0/weights/best_saved_model (28.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.8.2...\n",
            "Found untraced functions such as tf_conv_layer_call_fn, tf_conv_layer_call_and_return_conditional_losses, tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv2d_layer_call_fn while saving (showing 5 of 268). These functions will not be directly callable after loading.\n",
            "Assets written to: /tmp/tmpvbbxkjed/assets\n",
            "2022-09-25 19:13:22.749852: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
            "2022-09-25 19:13:22.749920: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
            "Estimated count of arithmetic ops: 18.905 G  ops, equivalently 9.452 G  MACs\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 56.2s, saved as runs/train/yolov5x_fold0/weights/best-fp16.tflite (14.2 MB)\n",
            "\n",
            "Export complete (67.5s)\n",
            "Results saved to \u001b[1m/content/yolov5/runs/train/yolov5x_fold0/weights\u001b[0m\n",
            "Detect:          python detect.py --weights runs/train/yolov5x_fold0/weights/best-fp16.tflite \n",
            "Validate:        python val.py --weights runs/train/yolov5x_fold0/weights/best-fp16.tflite \n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train/yolov5x_fold0/weights/best-fp16.tflite')\n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Penyimpanan Model"
      ],
      "metadata": {
        "id": "HJ7XjHGaj-lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk keperluan *deployment* baik secara lokal maupun secara *online*, kita akan unduh beban model yang sudah dilatih untuk kemudian diterapkan di tempat lain tanpa harus melatih kembali model tersebut."
      ],
      "metadata": {
        "id": "xPij7dhDj2Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/weights.zip /content/yolov5/runs/train/yolov5x_fold0/weights/\n",
        "\n",
        "files.download('/content/weights.zip')\n",
        "files.download('/content/yolov5/data/datacst.yaml')"
      ],
      "metadata": {
        "id": "ewjddiutGJcB",
        "outputId": "89b3b6bb-1d99-48a7-dc37-5f8e072bf5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/ (stored 0%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best-fp16.tflite (deflated 8%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best.pt (deflated 9%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/ (stored 0%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/variables/ (stored 0%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/variables/variables.index (deflated 33%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/variables/variables.data-00000-of-00001 (deflated 4%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/assets/ (stored 0%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/best_saved_model/saved_model.pb (deflated 11%)\n",
            "  adding: content/yolov5/runs/train/yolov5x_fold0/weights/last.pt (deflated 9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian kita akan coba lakukan *inferencing* dengan model yang telah kita latih."
      ],
      "metadata": {
        "id": "s3C4rWRYknfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --source /content/640_data_augment_yolo_light/images/train/amazon__1.png --weight /content/yolov5/runs/train/yolov5x_fold0/weights/best.pt --data /content/yolov5/data/datacst.yaml --name tes --conf 0.4"
      ],
      "metadata": {
        "id": "nPIdDCDaZ5hI",
        "outputId": "175dd487-c073-4ffc-f175-7a4b3a78bb59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/yolov5x_fold0/weights/best.pt'], source=/content/640_data_augment_yolo_light/images/train/amazon__1.png, data=/content/yolov5/data/datacst.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=tes, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ðŸš€ v6.2-164-g2787ad7 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 182 layers, 7346307 parameters, 0 gradients\n",
            "image 1/1 /content/640_data_augment_yolo_light/images/train/amazon__1.png: 640x640 1 Amazon, 14.2ms\n",
            "Speed: 0.6ms pre-process, 14.2ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/tes\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil *inference* dapat kita lihat di direktori yang tertera di baris terakhir eksekusi perintah tersebut.\n",
        "\n",
        "Berikut adalah langkah-langkah untuk langsung melakukan *inference* secara lokal secara singkat:\n",
        "\n",
        "1. Impor library YOLOv5 ke dokumen frontend,\n",
        "\n",
        "2. Masukkan dokumen yang ada di `weights.zip` ke `/yolov5/runs/train/yolov5x_fold0/weights`\n",
        "\n",
        "3. Masukkan `datacst.yaml` ke `/yolov5/data`"
      ],
      "metadata": {
        "id": "NGVZ6ovwk1Qu"
      }
    }
  ]
}